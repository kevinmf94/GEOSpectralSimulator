\documentclass[10pt,a4paper,twocolumn,twoside]{article}
\usepackage[utf8]{inputenc}
\usepackage[catalan]{babel}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{times}
\usepackage{titlesec}
\usepackage{multirow}
\usepackage{lettrine}
\usepackage[top=2cm, bottom=1.5cm, left=2cm, right=2cm]{geometry}
\usepackage[figurename=Fig.,tablename=TAULA]{caption}
\usepackage{hyperref}
\usepackage{listings}

\lstset{
  breaklines=true,
  tabsize=2,
}

\captionsetup[table]{textfont=sc}

\titlespacing*{\section}{0pt}{0.5cm}{0.2cm}
\titlespacing*{\subsection}{0pt}{0.5cm}{0.2cm}


\graphicspath{{img/}}

\author{\normalsize\sffamily Kevin Martín Fernández}
\title{\huge{\sffamily GEOSpectralSimulator}}
\date{}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

%No ident
\setlength\parindent{0pt}

%
%\large\bfseries\sffamily
\titleformat{\section}
{\large\sffamily\scshape\bfseries}
{\textbf{\thesection}}{1em}{}

%List counter enumerate
\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}
\renewcommand{\labelenumiii}{\theenumiii}
\renewcommand{\theenumiii}{\theenumii \arabic{enumiii}.}

\begin{document}

\fancyhead[LO]{\scriptsize Kevin Martín: GEOSpectralSimulator GEOSS}
\fancyhead[RO]{\thepage}
\fancyhead[LE]{\thepage}
\fancyhead[RE]{\scriptsize EE/UAB TFG INFORMÀTICA: GEOSpectralSimulator}

\fancyfoot[CO,CE]{}

\fancypagestyle{primerapagina}
{
   \fancyhf{}
   \fancyhead[L]{\scriptsize TFG EN ENGINYERIA INFORMÀTICA, ESCOLA D'ENGINYERIA (EE), UNIVERSITAT AUTÒNOMA DE BARCELONA (UAB)}
   \fancyfoot[C]{\scriptsize June 2019. Escola d'Enginyeria (UAB)}
}

%\lhead{\thepage}
%\chead{}
%\rhead{\tiny EE/UAB TFG INFORMÀTICA: TÍTOL (ABREUJAT SI ÉS MOLT LLARG)}
%\lhead{ EE/UAB \thepage}
%\lfoot{}
%\cfoot{\tiny{February 2015, Escola d'Enginyeria (UAB)}}
%\rfoot{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\pagestyle{fancy}

%\thispagestyle{myheadings}
\twocolumn[\begin{@twocolumnfalse}

{
\vspace*{-1cm}
\maketitle
}

\thispagestyle{primerapagina}
%\twocolumn[\begin{@twocolumnfalse}
%\maketitle
%\begin{abstract}
\begin{center}
\parbox{0.915\textwidth}
{\sffamily\small
\textbf{Abstract--} Resum
\\
\\
\textbf{Keywords-- }
Simulator, Terrain, Satelite, Multiespectre, Unreal Engine\\
\\
%\end{abstract}
}

\bigskip

{\vrule depth 0pt height 0.5pt width 4cm\hspace{7.5pt}%
\raisebox{-3.5pt}{\fontfamily{pzd}\fontencoding{U}\fontseries{m}\fontshape{n}\fontsize{11}{12}\selectfont\char70}%
\hspace{7.5pt}\vrule depth 0pt height 0.5pt width 4cm\relax}

\end{center}

\bigskip
%\end{abstract}
\end{@twocolumnfalse}]

\blfootnote{$\bullet$ E-mail de contacte: kevinmf94@gmail.com}
\blfootnote{$\bullet$ Menció realitzada: Enginyeria de Computació}
\blfootnote{$\bullet$ Treball tutoritzat per: Felipe Lumbreras Ruíz}
\blfootnote{$\bullet$ Curs 2018/19}

\vspace{-1cm}
\section{Introduction}
The simulation of the real world for the generation of synthetic images is a scope in which you are looking to represent the real world more accurately possible, in this way can generate synthetic information that represents real environments that can represent a danger (for the natural environment, people, etc) or a high economic cost.
\\
\\
On this project you want to get that is able to by means of elevation maps and aerial images of real terrains generates a simulated environment that it can generate its image datasets, which can be used in multiple areas of the computation learning. Also, it can simulate flying, see a geographical area or expand this project in other environments that uses a geographic information. That geographical information it can provided from multiple sources this allows to work with a lot of external data.

\section{Objectius}

En aquest apartat determinarem els diferents objectius del projecte en format de jerarquia per tal de veure la dependència entre els diferents objectius:

\begin{enumerate}
  \item Analitzar
  
  \item Definir
  \begin{enumerate}
    \item Definir mòduls pel projecte
    \item Definir l'estructura del software
    
    \item Definir plataformes utilitzades
    \begin{enumerate}
    	\item Definir els mòduls a desenvolupar
	    \item Definir la comunicació entre els mòduls
    	\item Definir estructura de les dades que rebrà Unreal Engine
  	\end{enumerate}
  \end{enumerate}
  
  \item Desenvolupar
  \begin{enumerate}
    \item Desenvolupar mòdul de transformació i obtenció de dades
    \item Desenvolupar mòdul gràfic (Ureal Engine)
    \begin{enumerate}
    	\item Desenvolupar la interfície del menú
	    \item Desenvolupar la lògica del vehicle
    	\item Desenvolupar codi per la carrega de terreny y material
    	\item Desenvolupar llibreria RPC per el control del entorn
  	\end{enumerate}
  	
  	\item Desenvolupar mòdul de scripting
  	\begin{enumerate}
    	\item Desenvolupament client que controlarà el vehicle
  	\end{enumerate}
  	
  	\item Integrar els mòduls d'AirSim en Unreal Engine
  	\begin{enumerate}
    	\item Integrar mòdul de Segmentació en el projecte
  	\end{enumerate}
  	
  	\item Desenvolupar altres capes d'informació
  \end{enumerate}
  
  \item Testejar
  \begin{enumerate}
    \item Fer provés del mòdul de transformació de dades
    \item Fer provés del mòdul gràfic
    \item Fer provés del mòdul de control per scripting
    \begin{enumerate}
    	\item Elaborar script d'exemple
	    \item Provar script d'exemple
  	\end{enumerate}
  \end{enumerate}
  
  \item Documentar
  \begin{enumerate}
    \item Redactar informe inicial
    \item Redactar informe de seguiment I
    \item Redactar informe de seguiment II
    \item Redactar l'informe final
    \item Elaborar proposta de presentació
    \item Elaborar pòster
    \item Gestionar la documentació del dossier
  \end{enumerate}
  
\end{enumerate}

\section{Methodology}
In this project its decide uses a methodology of Agile\cite{agile} style, this allows to identify more efficiently the little parts that compose the project, besides adapt to the changes. More concretely has used a technique called Kanban\cite{kanban} which consists in organizing this backlog (tasks of short duration) on cards that will be placed on a board according to the point of the life cycle on the tasks founds it. For this to be used the Trello\cite{trello} software that can see the boards on web browser, create cards and move it between different lists.

\subsection{Gant Diagram}

In order to manage a project you must also include a Gant diagram done with excel. In this diagram several tasks and subtasks are contemplated in order to make a prediction of the work done, the approximate time is what it will take for each task, this will allow an approximation of the time that the project behaves in this way.

\section{State of the art}
\label{estatart}

Currently exists diverse applications for the generation of images in simulated environments with the finality of generating data used in machine learning algorithms.

In this scope one of the most import is AirSim\cite{airsim} developed by Microsoft and Carla SIMULATOR\cite{carla} developed by ''Centre de Visio per Computador (CVC)'', also it's analyse others, how can they be LESS\cite{less}, DIRSIG {dirsig} or Google Earth Engine\cite{googleearth} of more specific scope.

\subsection{AirSim}
AirSim is a graphic simulator made with Unreal Engine, this simulator has the purpose of generating synthetic images on fake environments, it incorporates multiple modules that are offered the next functionalities (You can see an example in the figure \ref{fig-airsim}):

\begin{itemize}
  \item Simulation of cars.
  \item Simulation of drones.
  \item Compatibility with real drone controllers.
  \item Recording.
  \item Depth view.
  \item Segmentation view.
  \item Rain effects.
  \item Control of illumination according to the daily hours.
  \item Control of vehicles through a python script.
\end{itemize}

\begin{figure}[!h]
\centering
  	\includegraphics[width=0.4\textwidth]{airsim}
	\caption{AirSim simulator}
	\label{fig-airsim}
\end{figure}

\subsection{Carla SIMULATOR}
The Carla is a graphical simulator made on the Unreal Engine, this simulator has the finality of generating images on a fake environment with the maximum realism so that possible to generate images that they can serve to a made learning the neural networks able to drive safely a autonomous car taking into the unlikely cases that are difficult to generate in the real world. Its environment has the next functionalities:

\begin{itemize}
  \item Simulation of cars.
  \item Depth view.
  \item Segmentation view.
  \item Simulation of traffic, pedestrians, etc.
  \item Control of the actors (traffic, pedestrian, cameras, etc) with a python script.
\end{itemize}

\subsection{LESS}
LESS is a model of the radiation (You can see an example, in the figure \ref{fig-lessradiacio}) generated on a three-dimensional object/terrain for different rays, generating from a technique of ray-tracing its able to simulate data and images over realistic three-dimensional scenes. This model implements a method of follow weighted photons for simulating the effect of multi-spectral bidirectional reflectance.

\begin{figure}[!h]
\centering
  	\includegraphics[width=0.4\textwidth]{lessradiacio}
	\caption{Example of results of the radiation of a terrain}
	\label{fig-lessradiacio}
\end{figure}

\subsection{DIRSIG}
The model of Digital Imaging and Remote Sensing Image Generation (DIRSIG) is a model of generation of synthetic images developed by the lab of Digital Imaging and Remote Sensing of the Rochester Institute of Technology. The model can produce one band images, multispectral or hiperspectral from visible trough of the infrared thermal region of the electromagnetic spectral. You can see an example generated by DIRSIG on the figure \ref{fig-tacoma}.

\begin{figure}[!h]
\centering
  	\includegraphics[width=0.4\textwidth]{tacoma}
	\caption{Frame from the Tacoma port}
	\label{fig-tacoma}
\end{figure}

\subsection{Google Earth Engine}

Google Earth Engine is a project by Google, dedicated to offer the necessary tools to be able to analyse and visualize geospacial data, intended for an academy studies, non-profit institutions, companies and governments. The principal features of the Google Earth Engine are:

\begin{itemize}
  \item We can work with datasets from different satellites such as LANDSAT, MODIS, SENTINEL, etc.
  \item Incorporate a work environment to manipulate the data and wide API that allows us to combine images from different spectral, see it on a world map, export data to Google Drive and more.
\end{itemize}

\section{Structure of project}
In order to determine the project structure, it's study the diverse alternatives viewed on section \ref{estatart}. Will be analysed the structure from AirSim and Carla with the objective of deciding the ownership structure and the external libraries to incorporate them into the project.

\subsection{AirSim} 
AirSim is composed of multiple modules written in various languages, as can be seen below:

\begin{itemize}
	\item \textbf{AirLib (C++)}: Module for Unreal Engine that provides the base classes to communicate through the RPC protocol and control the simulated vehicles.
	\item \textbf{DroneServer (C++)}: Server to receive orders from RPC client.
  	\item \textbf{DroneShell (C++)}: Shell client to send orders to the Server.
  	\item \textbf{PythonClient (Python)}: Client to send orders trough RPC, also includes code to the manipulation of images.
  	\item \textbf{SGM (C++)}: Code to manipulate images and generate the segmentation view.
    \item \textbf{Unity (C\# i C++)}: Unity version, includes a modules to see the AirSim information.
    \item \textbf{Unreal Engine (C++)}: Unreal version, includes a modules to see the AirSim information.
\end{itemize}

\subsection{Carla SIMULATOR}

Carla SIMULATOR is composed for multiple modules as you can see in the figure \ref{fig-carlamodules} written in multiple languages. The modules of Carla are:

\begin{figure}[!h]
\centering
  	\includegraphics[width=0.5\textwidth]{carlamodules}
	\caption{Relation between the modules of Carla}
	\label{fig-carlamodules}
\end{figure}

\begin{itemize}
 \item \textbf{LibCarla (C++)}: The main library of Carla, is in charge of the logic of the simulation.
 \item \textbf{Unreal (C++)}: Graphic engine with the Carla plug-in, this includes all the functionalities added to Unreal.
 \item \textbf{PythonAPI (Python)}: The API allows sending orders to the Carla module that works like server, this API it's useful to make own scripts.
\end{itemize}


\subsection{The structure chosen}

Analysing multiple projects with similar features, it's decide for a own structure as we can see in the figure \ref{fig-dronsimulatormodules} being able to use something modules with other open-source projects. Its make this decision due to the fact that the other projects are based on the creation of terrain predefined on Unreal Editor, the opposite to the finality of this project in which its make terrain automatically provided by real data.

\begin{figure}[!h]
\centering
  	\includegraphics[width=0.45\textwidth]{structuretfg}
	\caption{Structure of GEOSpectralSimulator}
	\label{fig-dronsimulatormodules}
\end{figure}

Our project will consist of these modules:

\begin{itemize}
  \item \textbf{GEOTool (Python)}: This module has the finality of provides the needed tools for obtaining and adapt the data from some standards for geographical data web-service with the objective of can import this data on any graphic motor, it can generate diverse layers of data for the downloaded terrains as textures (RGB, infrared, etc.), geojson, etc. 
  
  \item \textbf{ScriptAPI (Python)}: This module provides the interface to control the simulator trough scripts and trajectories files, its allow to control the vehicle, cameras, when generate images for the dataset, etc. Also can implements extra functionality as generation of noise to the vehicle and more.
  
  \item \textbf{Unreal Engine Simulator (C++)}: Based on the Unreal Engine includes the plugins to implement an RPC Server, interface necessary to read files generated by GEOTool and the generation of images.
\end{itemize}

\section{Module GEOTool}

In this section can see how works the module GEOTool and which is the functionality implemented, more concretely can see the origin of data, the configuration file, how to prepare data for graphical engines like as Unreal, Unity, OpenGL, etc. Finally it will be analysed the performance of the generation from wavefront object files (.obj).

\subsection{Obtaining data}
\label{getdata}

With the objective of getting geoghrapical data it has been decided the communication with standards proposed by Open Geospatial Consortium\cite{ogc}: Web Map Service\cite{wms} in charge of make available image data how can it be orthophoto from a geographical region selected and Web Coverage Service\cite{wms} in charge of returns data concerning at the elevation of terrains in a concrete geographical region. In order to get it data they are made HTTP requests a the web services offered by multiple institutions that follows the standards called, in this case will be made tests with the Institut Cartogràfic I Geològic de Catalunya\cite{icgc}.

\subsubsection{Configuration file}

For determining which data will wish be obtained and where webservices the app accept by command line parameter a file with the configuration in JSON format. This allows determining the properties of the data they want to request as I can see on the appendix \ref{appendix:geotoolconfig}. On this file can configure the next parameters:

\begin{itemize}
  \item \textbf{Type}: It refers to the type of coordinates that we will pass, can be latlong or x-y, in the first case it will make the corresponding transformation to the UTM format (x-y).
  
  \item \textbf{Coordinates}: Coordinates on which we want to make the request in the format indicated in the type field. If "xy" is selected, the attributes x,y will be defined, and in case of choosing "latlong" we will define the lat, long attributes.

  \item \textbf{Dimensions}: The dimension field can choose the dimensions that you want to request in pixels, it is composed by:
  
  \begin{itemize}
    \item Bbox: Bbox is the dimensions in pixels of geographical area, this is used by all requests to identify the area in a unique way.
    \item Texture: Texture is a resolution in pixels request to the texture image.
  \end{itemize}
  
  \item \textbf{chunks}: The fragments that want to be downloaded, the application calculates the displacement and generates n pieces of width * height.
  
  \begin{itemize}
    \item width.
    \item height.
  \end{itemize}

  \item \textbf{cellsize}: Size of each pixel in meters.
  \item \textbf{meshStep}: The number indicates the quantity of points that wish to skip on the mesh (Default: 1). More information on the section \ref{qualitat}.
  \item \textbf{Wcsurl}: The URL to the web service that will give us the height data.
  \item \textbf{Outputwcs}: Base name of the output files.
  \item \textbf{Formatwcs}: Format of file generated by heights. Available: raw, obj (Object 3D).

  \item \textbf{Wmsrequests}: Array with each request that will be for obtaining textures, each item are composed by:
  
  \begin{itemize}
    \item Url: URL to the webservice WMS.
    \item Layers: The layers we want to get from these webservice.
    \item Name: The name of texture, used by the generated files.
    \item Format: Output format. Available: JPG.
  \end{itemize}
  
\end{itemize}

\subsection{Generation of terrains from a elevations maps}
In this section explain the multiple forms realized to generate terrains that can be interpreted with multiple graphic engines. More concretely can see the RAW format and Wavefront object (.obj).

\subsubsection{Format RAW}

The RAW format is a format plain based on values of 16 Bytes, where the value of sea is 128. All the heights are saved in binary format put in the file of form consecutive. This format is accepted by the Unreal and Unity land builder, but has dimension limitations; You must meet several specified conditions in the Unreal Editor, this causes you to lose control of the generated mesh, the texture coordinates do not match and the texture that is applied to the mesh will have to adapt. Motivation by which decide to add the generation of 3d objects in the standard format, generating own objects as you can see in section \ref{mesh3d}.

\subsubsection{Generation of mesh 3D}
\label{mesh3d}
To import land in the graphics engine, it decides to generate a 3D mesh in obj format compatible with any 3D editor, graphics engine, etc. This device gives the freedom to control the distance between the vertex, where the texture is applied and which is the normal vector of each vertex to correctly apply the algorithms of illumination.
\\
\\
As the treatment with loops is slow, it's made all the operations with the library NumPy take advantage of efficiency incorporates this library with the calculus of matrices. The problem has been adapted to operations of type matricial, you can see the code available in the annex \ref{appendix:generateobj}.
\\
\\
For the generation of objects must be defined four types of objects:

\begin{itemize}
  \item \textbf{Vertex}: Are each point in the world. To say what vertex are referenced are defined for the index according to read the elevation grid (1, 2, 3,..., H*W). Each point is multiplied by a K (K represents the distance between two vertex according to the distance that indicate the elevation map obtained).

  \item {
    \textbf{Vertex of texture}: Vertex with two components x and y compressed between 0 and 1 that indicates the correspondence between the points of the texture and location in the mesh. This property is calculated with the equations \ref{equation:u} and \ref{equation:v}.
    \begin{equation}
    \label{equation:u}
    u = f(column) = column / (width - 1)
    \end{equation}
    \begin{equation}
    \label{equation:v}
    v = f(row) = 1 - (row / (height - 1))
    \end{equation}
  }

  \item \textbf{Normal of vertex}: Are vectors indicates the direction in which reflects the light for each vertex of the object. In order to calculate these normals is necessary, calculate the normal for each face, these are not included in the final file because the engines generate it by default according to the order in which indicates the faces you can see later. 
  \begin{itemize}
    \item {
      Generation of normal faces: In order to generate the normals of a face once you know it the relation between the faces will be following the pattern you can see in the figure \ref{fig-normalcara} where will be follow the equation \ref{equation:u} for the calculation of the normal face. $\dot{\vec{A}}$, it makes cross product $\vec{C} = \vec{B}*\vec{A}$ and finally is normalize the vector $\vec{Normal} = \frac{\vec{C}}{\mid\vec{C}\mid}$.
    }
    \item {
		Generation of the normal for the vertex: In order to generate the vector normal for each vertex, it's using the structure that can see in figure \ref{fig-normalvertex} applying the next equation for each vertex where V correspond to the vertices and F correspond to the faces on the figure \ref{fig-normalvertex}:
      \begin{equation}
      \vec{NormalV} = \vec{F1} + \vec{F2} + \vec{F3} + \vec{F4} + \vec{F5} + \vec{F6}
      \end{equation}
      \begin{equation}
      \vec{NormalV} = \frac{\vec{NormalV}}{\mid\vec{NormalV}\mid}
      \end{equation}
    }
  \end{itemize}

  \item \textbf{Faces}: In this step it's determined which is the union of vertices to generate the different faces of the mesh, in this implementation it's decided to make a triangulation, in other words, that is for each square of the own mesh will be generated two triangular faces. It's important generate the faces in correct order writing the vertices in opposite clockwise, in this way the graphic motors determine that the normal face will point up by displaying the 3D mesh correctly.
\end{itemize}

\begin{figure}[!h]
\centering
  	\includegraphics[width=0.4\textwidth]{caranormal}
	\caption{Pattern for the calculation of normal on the faces}
	\label{fig-normalcara}
\end{figure}

\begin{figure}[!h]
\centering
  	\includegraphics[width=0.2\textwidth]{vertexnormal}
	\caption{Pattern for calculation of normal at a vertex}
	\label{fig-normalvertex}
\end{figure}

Once the generation process has been completed, the application will generate a mesh that can be opened in any editor. As it can see in the figure \ref{fig-meshlab} it's open in software MeshLab.

\begin{figure}[!h]
\centering
  	\includegraphics[width=0.35\textwidth]{mesh_example_meshlab}
	\caption{Terrain mesh visualized in software MeshLab}
	\label{fig-meshlab}
\end{figure}

\subsubsection{GEOJson associated at the terrain}

In order to can localize the terrain generated in other software that working with geospatial data or in the same simulator Unreal, it's decided to include in the generation a GEOJson that indicates the area which belongs the files generated. It's can see an example of this in the appendix \ref{appendix:geojson}.

\subsection{Time of generation}
In this section it's analyse the two versions implemented and can see the difference in time of generation. This is an important point for future implementations in which will wish implement a viewer in real time that loading new data according to the user moves for the world.
\\
On the figure \ref{fig-meshtime} can see the time spent from the version with loops for and the version implemented with NumPy, can see when realised the implementation using the library NumPy are reducing the time in 24x for a size of 500*500, this is due to the fact that NumPy is optimized for a parallelise the processing of data vectorial way.

\begin{figure}[!h]
\centering
  	\includegraphics[width=0.4\textwidth]{meshtime}
	\caption{Graph with the time of generation of a mesh according to size}
	\label{fig-meshtime}
\end{figure}

\subsection{Quality}
\label{qualitat}
In this section is looking the form in which reduce the quality of terrain and can see the results obtained both qualitatively (visual difference) as quantitatively (size in bytes). In order to make this reduction are made skips of size "meshStep" set in the JSON configuration. It's made tests with powers of 2 (2, 4, 8, 16,...) on the own matrix of points and using the terrain of size 300x300 pixels.
\\
\\
As we can see in the figure \ref{fig-qualitatmegas} when reducing the amount of points, we reduce the size of the disk by exponentially, making the file load faster on the Unreal environment. As it's can see in the figure \ref{fig-qualityvisual} is visible the loss of quality taking as a reference the mountain in the background on this see the loss of definition at the top of the mountain. It's can consider visually that when it is configured ''meshStep'' on 1 or 2 the loss qualitative, not are appreciable, from 4 are starting to appreciate slightly, finally on the levels 8 and 16 its can see the most loss, where the top of the mountain can see simplified the apposite that it can see with the ''meshStep'' configured to 1 where can see a mountain range with a good definition.

\begin{figure}[!h]
\centering
  	\includegraphics[width=0.4\textwidth]{qualitatmegas}
	\caption{Graphic with the size occupied on hard disk varying the ''meshStep'' parameter for a terrain of size 300x300.}
	\label{fig-qualitatmegas}
\end{figure}

\begin{figure}[!h]
\centering
  	\includegraphics[width=0.5\textwidth]{quality/quality}
	\caption{Visual comparison of the effects produced by the level quality}
	\label{fig-qualityvisual}
\end{figure}

\newpage
\section{Module: Simulator}

In this section its explain the multiple parts of which is composed the module of simulation. The objective of this module is to make available a tool that allows simulate a travel with a generic vehicle above a terrain getting images from multiple cameras configured on board of the vehicle. A way to make this is providing the interface for control the vehicle with an external script sending commands with the RPC Protocol, in this case, it's implemented the server as can see in section \ref{rpcserver} and control it with a client implemented in a module as can you see in the section \ref{modulescript}.

\subsection{Visualization and control of the vehicle}

This part is based on providing the interface for moving a generic vehicle to the simulated world, this vehicle is configured with several cameras that are displayed on the screen and that can generate images of each one of them, in this way the possibility is added to view the same area in multiple perspectives. As an example, you can see an zenith view in the figure \ref{fig-montserratir}.

\begin{figure}[!h]
\centering
  	\includegraphics[width=0.5\textwidth]{cenitalviewir}
	\caption{Infrared view of Montserrat in zenith position}
	\label{fig-montserratir}
\end{figure}

\subsubsection{Remote control}
\label{rpcserver}

In the order to controller remotely the vehicle, it's implemented a communication using the RPC protocol (Remote Procedure Call) more concretely it's using the library Rpclib\cite{rpclib} for C++. This server is implemented in the Unreal module, the RPC server management is done through a class (AVehiclePawn) that can be inherited so that it offers basically the interface necessary to control a vehicle, specifically the following actions is implemented:

\begin{itemize}
\item Initialize/stop the RPC server: Initialize the server by pressing the Y cloth to the simulator of Unreal and stop it with the U key.
\item Change the position of the vehicle.
\item Change rotation of the vehicle or cameras (Implemented with LookAt).
\item Ask for images from one of the cameras to be generated.
\end{itemize}

The assignment of the functionalities is done in the function ''void BindFunctions(rpc::server* server)'' which receives the instance of the server, this function can be overridden to later be added to legacy classes with more functionality as can be seen in the annex \ ref {appendix: extendrpc}. By default the AVehiclePawn class allows us to call the following RPC functions:

\begin{itemize}
\item setLocation(double x, double y, double z): Send the location on which one place the vehicle.

\item setLookAt(double x, double y, double z): Send a vector with the position in the world you want to look at, so that at which point the simulator should keep the camera's eyes.

\item setLocationAndLookAt(double x, double y, double z, double lx, double ly, double lz): The function that implements the two previous calls in a single call.

\item setCameraLookAt(int cameraId, double x, double y, double z): A function that choose a camera and configure in what direction the camera look at.

\item getImage(int idCamera, std::string path): It allows us to indicate which camera and where we want to save the image.
\end{itemize}

\subsubsection{Sky visualization}
In order to implement a sky, it's used the native module of Unreal that's allowed to generate a sphere in which its render a sky. This sky has multiple parameters for configuring the sky allows a multiple situation, some of the things that can be done are: determine the sun position, the lighting of stars, the quantity of clouds, the velocity of clouds, etc.
\\\\
This allows to generate synthetic images in different moments of the day with a different illuminations, allow to generate more complex datasets. On the other hand, this implies that it is applied without shadows because the shadows are generated by Unreal, if these shadows are generated by the 2 bands, they overlap generating an undesirable strange effect. Its can see an example of differents hours of the day in the figure \ref{fig-sky}.

\begin{figure}[!h]
\centering
  	\includegraphics[width=0.48\textwidth]{sky/sky}
	\caption{Example of sky}
	\label{fig-sky}
\end{figure}

\subsection{Terrain}
The terrain is loaded through code executed at runtime obtaining the obj file and process this file to generate a UProceduralMesh\cite{uprocedural} that contains the mesh generated with the api of this class to generate meshes in runtime. To generate this mesh must be considered that Unreal works with a different system of coordinates, concretely is inverted respect to the UTM model on the y axis, in this way, it is necessary to make the necessary changes.
\\
\\
This mesh is loaded on the world and its represented by the class ''MapChunk'' it which is assigned a dynamic material that contains the loaded textures for generate the different visualizations. The texture are can be any image prepared to use as a texture on Unreal, giving the possibility of seeing RGB, infrared, multispectral bands, indexes generated by the multispectral data, etc.

\subsection{Results of visualization of terrains}
In this section can see the results obtained from the 3D terrains generated by the module GEOTool and apply multiple visualizations from origins as textures or shaders obtained from different sources. In concretely the view RGB and infrared is from ''Institut Cartogràfic de Catalunya'', the multispectral data are obtained from the browser EO Browser\cite{eobrowser} and the depth data is generated applying a shader to the scene.
\\
\\
You can see in Figure \ref{fig-bands} how it has been applied to a terrain that corresponds to "Sales de Pallars" different multispectral bands obtained from the satellite Sentinel 2\cite{sentinel2}. The first three bands B02, B03 and B04 correspond to the bands of colours, the band B05 corresponds to the fast change of the reluctance that produces in the vegetation in a range near of infrared, the band B08 obtain NIR\cite{nir} data and the B09 is able to detect water vapours.

This way with these bands can obtain composed data from the multiple spectrum that can apply to multiple scopes. In the figure \ref{fig-spectralindexes} can see some examples, between them can see the indexes they are:

\begin{itemize}
\item 
{
	NVDI\cite{ndvi}: Based on the combination of the bands (B08 - B04)/(B08 + B04), index used to see the farming state.
}
\item
{
	Moisture index\cite{moisture}: Based on the combination of the bands (B8A - B11)/(B8A + B11), indicates the proportion of precipitation that is needed so that satisfy the needs of vegetation.
}
\item
{
	NDWI\cite{ndwi}: Based on the combination of the bands (B03 - B08)/(B03 + B08), the index is used to determine the hydrique stress of the vegetation, saturation of the humidity in the land or limit the mass of water a lakes or reservoirs.
}
\end{itemize} 

The last view of the figure \ref{fig-spectralindexes} can see the depth or proximity used by algorithms of 3D reconstruction with monocular stereo, prevention of collisions, algorithm of multiview stereo, etc.

\begin{figure*}[!h]
\centering
  	\includegraphics[width=1\textwidth]{multispectral/bands}
	\caption{View of ''Sales de Pallars'' in six spectral bands differents.}
	\label{fig-bands}
\end{figure*}

\begin{figure*}[!h]
\centering
  	\includegraphics[width=1\textwidth]{multispectral/spectralindexes}
	\caption{View of ''Sales de Pallars'' in RGB, infrared, NDVI, moisture, NDWI and depth.}
	\label{fig-spectralindexes}
\end{figure*}

\subsection{Interacció amb el simulador}
In order to interact with the simulator the following keys have been defined:

\begin{itemize}
\item A,W,S,D: Left, Front, Back, Right movement of the vehicle.
\item Alt, SpaceBar: Down and up the vehicle.
\item Q,E: Left and right rotation of vehicle.
\item Y: Starts an RPC Server.
\item U: Stops an RPC Server.
\item H: Show/Hide the visualization of vehicle.
\item K: Save image in the simulator folder from the second camera.
\end{itemize}

\section{Scripting module}
\label{modulescript}

In this section it is explained and see the principal points of the scripting module in which can control the simulator in a way that can be reproduce travels, reading trajectories from files, generate images, generate noise to the trajectories, etc.

\subsection{Trajectories}
The module of scripting allow to make travel, reading trajectories created in a real world (adapt this to the own format) or generated synthetic this way can recreate on the simulator a trajectory done previously were able reproduce so many times, this allows to do multiple tests with different visualizations of the terrain. For this finally its generate a file that explained in the section \ref{file-trajectories}.

\subsubsection{Fitxer de trajectòries}
\label{file-trajectories}
This section explains how to form the path file and the file to place the cameras. The file is in CSV format and can be read from the GEOControl module to indicate to the simulator the positions of the vehicle and where it looks both the vehicle and the cameras in coordinates of the world.

The file controls the vehicle are composed by the next fields:

\begin{itemize}
\item Time: Time in milliseconds from start of the script.
\item x: Position X in UTM format.
\item y: Position Y in UTM format.
\item z: Position Z in UTM format.
\item LookX: Position X to which we look in UTM format.
\item LookY: Position Y to which we look in UTM format.
\item LookZ: Position Z to which we look in UTM format.
\end{itemize}

The file controls the cameras are in another file composed of the fields:

\begin{itemize}
\item Time: Time in milliseconds from start of the script.
\item cameraId: The Id of the camera that want to modify.
\item LookX: Position X to which we look in UTM format.
\item LookY: Position Y to which we look in UTM format.
\item LookZ: Position Z to which we look in UTM format.
\item GetImage: Boolean (0 or 1) that indicates if you want to generate an image from this camera in this instant of time.
\end{itemize}

It can see an example from the CSV files in the appendix \ref{appendix:fitxerscsv} generates with Excel.

\subsection{Noise simulation}

As vehicles have unexpected movements due to the wind, the condition of the asphalt among others, is implementing a generator of noise applied to the trajectory you want to play, this way they are can generate images with noise and unexpected movements. This noise is implemented through a Gaussian noise in which the point at which you want to move is used as the center and the sigma is small (between 0 and 1) that generate a lightweight movement.

\subsection{Generation of Datasets}
One of the objectives of this project is giving the possibility of generating datasets of synthetic images with data from satellite, in order to be able generate datasets use the field ''GetImage'' seen in the section \ref{file-trajectories} that generate an image every time he find this field.

In the figure \ref{fig-dataset}  can see a little example with a few images of a tour in which its fixed the camera with a concrete point that it is left behind with the pass of time.

\begin{figure}[!h]
\centering
  	\includegraphics[width=0.45\textwidth]{dataset/dataset}
	\caption{Set of images generated by GEOControl}
	\label{fig-dataset}
\end{figure}

\section{Conclusions}

In this project its can see as it is can obtain textures and elevation maps of sources coming of satellites and manipulate this data for generating composed data, indexes, new data generated by neural networks, etc. Once data is treated are converted to 3D so that it is can add on a graphical engine as is Unreal Engine. It has been taken into account aspects as the velocity in generating a 3D model where can see the effect of the libraries as NumPy in the manipulation of matrices, accelerating the process thanks to the parallelism. It is seeing the effect quantitatively and qualitatively in the meshes when reduces the quantity of points with the which is generating the model 3D needed for render models of big dimensions.

The section of the simulator module its can see as can visualize the data in an environment 3D generated by Unreal Engine. In the simulator can add cameras a vehicle simulated and see in different perspectives the same data for this its created a module that sends commands to an RPC server that implemented in Unreal Engine in which can move, tell where to look the vehicle, where to look the cameras and get images of this camera.
The tests has been with several satellite information from Sentinel 2, which has been seen in what is represented in the simulated world and what uses has this information and can see the possibilities offers the simulator.

In the part of the module of scripting its can see as can generate trajectories reproducible multiple times on the environment simulated in a way that can make multiple travels with different information or the same information viewed from another point of view and extract data in the form of datasets. These trajectories can control multiple parameters of the vehicle and the cameras that indicate in which points they want to obtain an image and which camera. In the real world the vehicles have a movement, unexpected (the wind, the asphalt and other factors) for this reason is added a little filter as the generation of noise provocated an unexpected movement of the vehicle and the camera on-board of this.


\section*{Acknowledgments}

First of all I want to thanks to my tutor Felipe Lumbreras for the time spent in this project, the help to consult to experts and all the ideas generated on the reunions, in second thanks to Marc(CVC)... from guide me with the Unreal Engine and the best way to realize this project successful and for last thanks a my friend Javi Cantero for the help to check the English version of this document.

\begin{thebibliography}{11}
\bibitem{agile}
Agile software development
\\ \url{https://en.wikipedia.org/wiki/Agile_software_development}
[19/02/2019]
 
\bibitem{kanban}
Kanban
\\ \url{https://www.iebschool.com/blog/metodologia-kanban-agile-scrum/} [19/02/2019]

\bibitem{trello}
Trello - \url{https://trello.com/} [19/02/2019]

\bibitem{airsim}
AirSim - \url{https://github.com/Microsoft/AirSim} [19/02/2019]

\bibitem{carla}
Carla SIMULATOR - \url{http://carla.org} [19/02/2019]

\bibitem{less}
LESS - \url{http://lessrt.org/} [11/04/2019]

\bibitem{dirsig}
Digital Imaging and Remote Sensing Image Generation - \url{http://dirsig.org/} [11/04/2019]

\bibitem{googleearth}
Google Earth Engine - \url{https://earthengine.google.com/} [20/05/2019]

\bibitem{unreal}
Unreal Engine - \url{https://www.unrealengine.com/en-US/what-is-unreal-engine-4} [09/03/2019]

\bibitem{ogc}
Open Geospatial Consortium (OGC) -  \url{http://www.opengeospatial.org/} [08/04/2019]

\bibitem{wms}
Web Map Service (WMS) -  \url{https://www.opengeospatial.org/standards/wms} [08/04/2019]

\bibitem{wcs}
Web Coverage Service (WCS) -  \url{https://www.opengeospatial.org/standards/wcs} [08/04/2019]

\bibitem{icgc}
Institut Cartogràfic i Geològic de Catalunya (ICGC) - \url{http://www.icgc.cat/ca/} [08/04/2019]

\bibitem{wavefrontobj}
Wavefront .obj file - \url{https://en.wikipedia.org/wiki/Wavefront_.obj_file} [09/04/2019]

\bibitem{rpclib}
RPC Lib - Modern mgspack-rpc for C++ - \url{http://rpclib.net/} [10/04/2019]

\bibitem{uprocedural}
Procedural Mesh Component - \url{https://wiki.unrealengine.com/Procedural_Mesh_Component_in_C%2B%2B:Getting_Started} [21/05/2019]

\bibitem{eobrowser}
EO Browser - \url{https://apps.sentinel-hub.com/eo-browser/} [25/05/2019]

\bibitem{sentinel2}
Sentinel 2 - \url{https://www.esa.int/esl/ESA_in_your_country/Spain/SENTINEL_2} [25/05/2019]

\bibitem{nir}
Tecnología NIR, sus Usos y Aplicaciones - \url{https://www.engormix.com/balanceados/articulos/tecnologia-nir-sus-usos-t32534.htm} [25/05/2019]

\bibitem{ndvi}
El NDVI o Índice de vegetación de diferencia normalizada - \url{https://geoinnova.org/blog-territorio/ndvi-indice-vegetacion/} [25/05/2019]

\bibitem{moisture}
Moisture index - \url{http://glossary.ametsoc.org/wiki/Moisture_index} [25/05/2019]

\bibitem{ndwi}
Cálculo del índice NDWI - \url{http://www.gisandbeers.com/calculo-del-indice-ndwi-diferencial-de-agua-normalizado/} [25/05/2019]

\end{thebibliography}

\appendix
\section*{Apèndix}

\setcounter{section}{1}

\subsection{JSON d'exemple per la configuració de GEOTool}
\label{appendix:geotoolconfig}
\lstinputlisting{geotoolconfig.json}

\subsection{Codi per la generació d'una malla 3D a partir d'un fitxer d'altures}
\label{appendix:generateobj}
\lstinputlisting[language=Python]{generateobj.py}

\subsection{GEOJson d'exemple}
\label{appendix:geojson}
\lstinputlisting{geojson.json}

\subsection{Codi d'exemple per estendre la funcionalitat RPC}
\label{appendix:extendrpc}

\lstset{language=C} 
\begin{lstlisting}
.h:
	virtual void BindFunctions(rpc::server* server) override;
	
.cpp:
void MyClass::BindFunctions(rpc::server* server)
{
	Super::BindFunctions(server);

	server->bind("nameOfFunction", [context_params](Variables...) {
		//MyCode
	});
}

\end{lstlisting}

\subsection{Fitxers CSV d'exemple per controlar el vehicle i les càmeres}
\label{appendix:fitxerscsv}

\begin{figure}[!h]
\centering
  	\includegraphics[width=0.45\textwidth]{fitxervehicle}
	\captionsetup{labelformat=empty}
	\caption{Fitxer CSV per controlar el vehicle}
	\label{fig-fitxervehicle}
\end{figure}


\begin{figure}[!h]
\centering
  	\includegraphics[width=0.45\textwidth]{fitxercameres}
  	\captionsetup{labelformat=empty}
	\caption{Fitxer CSV per controlar les càmeres}
	\label{fig-fitxercameres}
\end{figure}


\end{document}

